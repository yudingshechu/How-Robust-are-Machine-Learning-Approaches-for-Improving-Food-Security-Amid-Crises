{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing data for analysis\n",
    "Here we generate the training and testing data for analysis. \n",
    "We will generate the standardized training data, resampled training data and standardized testing data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_basic feature engineering_\n",
    "\n",
    "here we create new variables: \n",
    "\n",
    "*FCSStaus_lag*: regional average food insecure proportion for UNHS 2016, 1146 obs with NA because the district level mismatch. \n",
    "\n",
    "*FoodInsecureMonthly_lag*: monthly average food insecure proportion for UNHS 2016. \n",
    "\n",
    "Other discrete variables are coded as frequency or dummy variables. We also added some log transformation variables for skewed distributed variables. \n",
    "\n",
    "_imputation_\n",
    "\n",
    "we use mode imputation to keep the number of observations large and to avoid some errors in feature engineering problems. This approach has advantage that we keep as much as information we can in our analysis, but with a disadvantage that we might introduced some noise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "data2019 = pd.read_csv(\"./bld/datasets/data2019.csv\")\n",
    "data2016 = pd.read_csv(\"./bld/datasets/data2016.csv\")\n",
    "\n",
    "# imputation for NA, with mode  \n",
    "individual = ['HouseType', 'RoofType', 'WaterSource', \n",
    "       'DistDrinkingWater', 'ShareToilet', 'Income', 'Kind.Income.Ratio', 'Salt', 'SubjectivePoverty', 'RelLivStandard',\n",
    "       'IncomeStab', 'LivStandChange','MaleRatio', 'AvgAge', 'SelfArg', 'SelfHerd',\n",
    "       'OwnNow_ArgLand', 'valueNow_ArgLand', 'Own1yrAgo_ArgLand',\n",
    "       'ValueAgo_ArgLand', 'OwnNow_TV', 'valueNow_TV', 'Own1yrAgo_TV',\n",
    "       'ValueAgo_TV', 'OwnNow_FixPhone', 'valueNow_FixPhone',\n",
    "       'Own1yrAgo_FixPhone', 'ValueAgo_FixPhone', 'OwnNow_MobilePhone',\n",
    "       'valueNow_MobilePhone', 'Own1yrAgo_MobilePhone', 'ValueAgo_MobilePhone','OwnNow_Refrigerator', 'valueNow_Refrigerator',\n",
    "       'Own1yrAgo_Refrigerator', 'ValueAgo_Refrigerator', 'OwnNow_Furniture',\n",
    "       'valueNow_Furniture', 'Own1yrAgo_Furniture', 'ValueAgo_Furniture','OwnNow_Cooker', 'valueNow_Cooker', 'Own1yrAgo_Cooker',\n",
    "       'ValueAgo_Cooker', 'OwnNow_Livestock', 'valueNow_Livestock','Own1yrAgo_Livestock', 'ValueAgo_Livestock','valueNowTotal',\n",
    "       'ValueAgoTotal','FamilySize','SelfStapleTypes'] # ShareToilet and Salt are dummies \n",
    "\n",
    "for i in individual: \n",
    "#     data2019[i].fillna(data2019[i].mode()[0], inplace=True)\n",
    "    data2019.fillna({i: data2019[i].mode()[0]}, inplace=True)\n",
    "\n",
    "\n",
    "# generate lagged variables of monthly mean FCS\n",
    "data2016.rename(columns={'mean_FCS':'FoodInsecureMonthly_lag'}, inplace=True)\n",
    "unique_month_mean_FCS = data2016[['month', 'FoodInsecureMonthly_lag']].drop_duplicates()\n",
    "data2019 = pd.merge(data2019, unique_month_mean_FCS, on='month', how='left', suffixes=('', '_unique'))\n",
    "# create a lagged regional mean FCS\n",
    "data2016['dismerge'] = data2016['s1aq2a'].str.upper()\n",
    "mg2016 = data2016[['FCSStaus', 'dismerge']].groupby('dismerge').mean().reset_index()\n",
    "mg2016.rename(columns={'dismerge':'s1aq2a', 'FCSStaus':'FCSStaus_lag'}, inplace=True)\n",
    "data2019['FCSStaus_lag'] = pd.merge(data2019, mg2016, on='s1aq2a', how='left')['FCSStaus_lag']\n",
    "# data2019['FCSStaus_lag'].fillna(data2019['FCSStaus_lag'].mode()[0], inplace=True) # impute with mode\n",
    "data2019.fillna({'FCSStaus_lag': data2019['FCSStaus_lag'].mode()[0]}, inplace=True) # impute with mode\n",
    "\n",
    "wkd2019 = pd.get_dummies(data2019,columns=[\"ShareToilet\",\"Salt\"])\n",
    "wkd2019_2 = pd.get_dummies(wkd2019,columns=[\"IncomeStab\",'SubjectivePoverty', 'RelLivStandard','LivStandChange'])\n",
    "\n",
    "# frequency encoding \n",
    "freq = ['HouseType', 'RoofType', 'WaterSource']\n",
    "wkd2019_2['DistDrinkingWaterBig3'] = wkd2019_2['DistDrinkingWater'].replace({'0-3': 0, \"3-5\": 1, \"5-8\": 1, '8 or more KMs': 1})\n",
    "wkd2019_2['DistDrinkingWaterBig3'] = wkd2019_2['DistDrinkingWaterBig3'].infer_objects(copy=False)\n",
    "\n",
    "for j in freq: \n",
    "    freqf = wkd2019_2.groupby(j).size()/len(wkd2019_2)\n",
    "    wkd2019_2[f\"{j}_feq\"] = wkd2019_2[j].apply(lambda x : freqf[x])\n",
    "    \n",
    "wkd2019_2['fatalitiesMean'] = wkd2019_2[['fatalities', 'fatalities.lag1', 'fatalities.lag2', 'fatalities.lag3',\n",
    "       'fatalities.lag4']].sum(axis=1)\n",
    "wkd2019_2['temperatureMean'] = wkd2019_2[['temperature', 'temperature.lag1',\n",
    "       'temperature.lag2', 'temperature.lag3', 'temperature.lag4']].mean(axis=1)\n",
    "wkd2019_2['precipitationMean'] = wkd2019_2[['precipitation', 'precipitation.lag1', 'precipitation.lag2',\n",
    "       'precipitation.lag3', 'precipitation.lag4']].mean(axis=1)\n",
    "wkd2019_2['NDVIMean'] = wkd2019_2[['NDVI', 'NDVI.lag1', 'NDVI.lag2', 'NDVI.lag3', 'NDVI.lag4']].mean(axis=1)\n",
    "wkd2019_2['NDVI.Anomaly.Mean'] = wkd2019_2[['NDVI.Anomaly',\n",
    "       'NDVI.Anomaly.lag1', 'NDVI.Anomaly.lag2', 'NDVI.Anomaly.lag3',\n",
    "       'NDVI.Anomaly.lag4']].mean(axis=1)\n",
    "\n",
    "# feature engineering for log transformation\n",
    "value_list = ['valueNow_ArgLand', \"Income\", \n",
    "       'valueNow_TV', 'valueNow_FixPhone',  'valueNow_MobilePhone', 'valueNow_Refrigerator', 'valueNow_Furniture', \n",
    "       'valueNow_Cooker', 'valueNow_Livestock', 'valueNowTotal', 'ValueAgoTotal']\n",
    "value_list_new = [f'{i}_new' for i in value_list]\n",
    "wkd2019_2[value_list_new] = np.log(wkd2019_2[value_list] + 1)*10\n",
    "wkd2019_2['FamilySize_new'] =  np.log(wkd2019_2['FamilySize'])*10\n",
    "wkd2019_2['SelfStapleTypes_new'] =  np.log(wkd2019_2['SelfStapleTypes'] + 1)*10\n",
    "\n",
    "macrologList = ['NL_District', 'NL_County', 'fatalitiesMean', 'temperatureMean',\n",
    "       'precipitationMean', 'NDVIMean', 'NDVI.Anomaly.Mean', 'Kind.Income.Ratio', \n",
    "       'Average..mm.', 'X1.Month.Anomaly....', 'X3.Months.Anomaly....','MaleRatio', 'AvgAge' ]\n",
    "macrologList_new = [f'{i}_log' for i in macrologList]\n",
    "wkd2019_2[macrologList_new] = np.log(wkd2019_2[macrologList]+1)*10\n",
    "\n",
    "time_splitted_data_1920 = { }\n",
    "for y in [2019, 2020]:\n",
    "    for m in range(1,13):\n",
    "        if wkd2019_2.query(\"year == @y and month == @m\").shape[0] != 0: \n",
    "            time_splitted_data_1920[f\"{y}_{m}\"] = wkd2019_2.query(\"year == @y and month == @m\").reset_index(drop = True)\n",
    "        else: \n",
    "            continue \n",
    "        \n",
    "#  FoodInsecureMonthly_lag\n",
    "predictorList = ['FCSStaus_lag', 'urban','NL_District_log', 'FoodInsecureMonthly_lag', \n",
    "       'precipitationMean', 'NDVI.Anomaly.Mean',\n",
    "       'Average..mm.',  'X1.Month.Anomaly....', 'X3.Months.Anomaly....', \n",
    "       'fatalitiesMean_log','temperatureMean_log','NDVIMean_log',\n",
    "       'Kind.Income.Ratio','MaleRatio', 'AvgAge', 'SelfArg', 'SelfHerd', 'ShareToilet_Yes', 'Salt_Yes',\n",
    "       'HouseType_feq', 'RoofType_feq', 'WaterSource_feq', 'IncomeStab_Somewhat stable',\n",
    "       'IncomeStab_Very unstable',\n",
    "       'SubjectivePoverty_Neither poor nor rich', 'SubjectivePoverty_Poor',\n",
    "       'SubjectivePoverty_Very poor', 'RelLivStandard_Better off',\n",
    "       'RelLivStandard_Same', 'RelLivStandard_Worse off',\n",
    "       'LivStandChange_Decreased', 'LivStandChange_Increased',\n",
    "       'LivStandChange_Stayed at the same', 'DistDrinkingWaterBig3', 'FamilySize', \n",
    "       'SelfStapleTypes', 'valueNow_MobilePhone', 'valueNowTotal', 'valueNow_Furniture', \n",
    "        'valueNow_MobilePhone_new', 'valueNow_ArgLand_new', 'valueNow_ArgLand', 'valueNow_Livestock', \n",
    "       'valueNow_Furniture_new', 'valueNow_Livestock_new', 'valueNowTotal_new', 'Income_new', 'Income', \n",
    "       'ValueAgoTotal_new', 'valueNow_FixPhone',  \n",
    "       'valueNow_Refrigerator']\n",
    "\n",
    "binaryList = ['urban','Salt_Yes','ShareToilet_Yes','SelfArg', 'SelfHerd', 'IncomeStab_Somewhat stable',\n",
    "       'IncomeStab_Very unstable','SubjectivePoverty_Neither poor nor rich', 'SubjectivePoverty_Poor',\n",
    "       'SubjectivePoverty_Very poor', 'RelLivStandard_Better off',\n",
    "       'RelLivStandard_Same', 'RelLivStandard_Worse off',\n",
    "       'LivStandChange_Decreased', 'LivStandChange_Increased', \n",
    "       'LivStandChange_Stayed at the same','DistDrinkingWaterBig3']\n",
    "\n",
    "freqList = [f\"{i}_feq\" for i in freq]\n",
    "nonstdList = []\n",
    "\n",
    "standardizationList = list(set(predictorList) - set(binaryList) - set(freqList) - set(nonstdList))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shap\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from python_functions.modues import StandardizerTrainTest, resampling, standardiza_resample_data_reindex\n",
    "from python_functions.modues import generate_traintest_data_by_time, standardiza_resample_data # here put train test splitting function \n",
    "from sklearn.utils import resample\n",
    "\n",
    "# create splitted train and test base datasets separately by time \n",
    "# _non means the data is not splitted by region, just like the original data\n",
    "# _district means the data is splitted by district, for the training, the district is not 2 and covid is 0, \n",
    "# for the testing, the district is 2\n",
    "trainData_non, testData_non = generate_traintest_data_by_time(time_splitted_data_1920, None)\n",
    "trainData_district, testData_district = generate_traintest_data_by_time(time_splitted_data_1920, 'District_covid')\n",
    "trainData_county, testData_county = generate_traintest_data_by_time(time_splitted_data_1920, 'county_covid')\n",
    "trainData_subcounty, testData_subcounty = generate_traintest_data_by_time(time_splitted_data_1920, 'subcounty_covid')\n",
    "# Filter non-zero rows for testData and keep specific trainData\n",
    "def filter_non_zero_rows(data_dict):\n",
    "    filtered_data = {}\n",
    "    for key, df in data_dict.items():\n",
    "        if 'test' in key:\n",
    "            num_rows = df.shape[0]\n",
    "            if num_rows != 0:\n",
    "                filtered_data[key] = df\n",
    "            else:\n",
    "                continue\n",
    "        elif key in ['train_2', 'train_3', 'train_4', 'train_5', 'train_10']:\n",
    "            filtered_data[key] = df\n",
    "    return filtered_data\n",
    "\n",
    "testData_district_filtered = filter_non_zero_rows(testData_district)\n",
    "testData_county_filtered = filter_non_zero_rows(testData_county)\n",
    "testData_subcounty_filtered = filter_non_zero_rows(testData_subcounty)\n",
    "\n",
    "trainData_district_filtered = filter_non_zero_rows(trainData_district)\n",
    "trainData_county_filtered = filter_non_zero_rows(trainData_county)\n",
    "trainData_subcounty_filtered = filter_non_zero_rows(trainData_subcounty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_Train test split_\n",
    "\n",
    "Here *StdTrain_district* contains all pre-Covid data splitted and add up by month, they only contains which District_covid != 2 and covid == 0 households, to avoid data leakage. \n",
    "\n",
    "*StdTest_district* contains all during-Covid monthly and total testing data, they only contrains which District_covid == 2 and covid == 1 households. \n",
    "\n",
    "Other datasets follow the same rules. and _non data sets are just as same as the origin datasets. \n",
    "\n",
    "Only difference between *StdTrain_non* and *StdTrain_region* is that: *StdTrain_non* contains the updated training data during Covid. \n",
    "\n",
    "Only difference between *StdTest_non* and *StdTest_region* is that: *StdTest_non* contains the overlapped regions before and during Covid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time series data for different region levels: during COVID data \n",
    "StdTrain_non, StdTest_non, SMOTE_Train_non, ADASYN_Train_non, SMOTETOM_Train_non, SMOTEENN_Train_non = standardiza_resample_data(trainData_non, testData_non, predictorList, standardizationList, binaryList, freqList, nonstdList)\n",
    "StdTrain_district, StdTest_district, SMOTE_Train_district, ADASYN_Train_district, SMOTETOM_Train_district, SMOTEENN_Train_district = standardiza_resample_data(trainData_district_filtered, testData_district_filtered,  predictorList, standardizationList, binaryList, freqList, nonstdList)\n",
    "StdTrain_county, StdTest_county, SMOTE_Train_county, ADASYN_Train_county, SMOTETOM_Train_county, SMOTEENN_Train_county = standardiza_resample_data(trainData_county_filtered, testData_county_filtered, predictorList, standardizationList, binaryList, freqList, nonstdList)\n",
    "StdTrain_subcounty, StdTest_subcounty, SMOTE_Train_subcounty, ADASYN_Train_subcounty, SMOTETOM_Train_subcounty, SMOTEENN_Train_subcounty = standardiza_resample_data(trainData_subcounty_filtered, testData_subcounty_filtered, predictorList, standardizationList, binaryList, freqList, nonstdList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_during_district = {'StdTrain_district': StdTrain_district, 'StdTest_district': StdTest_district, 'SMOTE_Train_district': SMOTE_Train_district, 'ADASYN_Train_district': ADASYN_Train_district, 'SMOTETOM_Train_district': SMOTETOM_Train_district, 'SMOTEENN_Train_district': SMOTEENN_Train_district}\n",
    "data_during_county = {'StdTrain_county': StdTrain_county, 'StdTest_county': StdTest_county, 'SMOTE_Train_county': SMOTE_Train_county, 'ADASYN_Train_county': ADASYN_Train_county, 'SMOTETOM_Train_county': SMOTETOM_Train_county, 'SMOTEENN_Train_county': SMOTEENN_Train_county}\n",
    "data_during_subcounty = {'StdTrain_subcounty': StdTrain_subcounty, 'StdTest_subcounty': StdTest_subcounty, 'SMOTE_Train_subcounty': SMOTE_Train_subcounty, 'ADASYN_Train_subcounty': ADASYN_Train_subcounty, 'SMOTETOM_Train_subcounty': SMOTETOM_Train_subcounty, 'SMOTEENN_Train_subcounty': SMOTEENN_Train_subcounty}\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = 'bld/datasets/generated/'\n",
    "\n",
    "# Store each dictionary as a pickle file\n",
    "with open(f'{output_dir}data_during_district.pkl', 'wb') as f:\n",
    "    pickle.dump(data_during_district, f)\n",
    "with open(f'{output_dir}data_during_county.pkl', 'wb') as f:\n",
    "    pickle.dump(data_during_county, f)\n",
    "with open(f'{output_dir}data_during_subcounty.pkl', 'wb') as f:\n",
    "    pickle.dump(data_during_subcounty, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlapping elements found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract unique counties\n",
    "unique_counties = trainData_county_filtered['train_10']['County'].unique()\n",
    "\n",
    "# Split the counties into train and test sets\n",
    "train_counties, test_counties = train_test_split(unique_counties, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create train and test sets based on the split counties\n",
    "train_data_county = trainData_county_filtered['train_10'][trainData_county_filtered['train_10']['County'].isin(train_counties)]\n",
    "test_data_county = trainData_county_filtered['train_10'][trainData_county_filtered['train_10']['County'].isin(test_counties)]\n",
    "\n",
    "trainData_county_before = {'train_10': train_data_county[predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid']]}\n",
    "testData_county_before = {'test_10': test_data_county[ predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid']]}\n",
    "\n",
    "overlap = set(train_counties) & set(test_counties)\n",
    "if overlap:\n",
    "    print(\"Overlapping elements found:\", overlap)\n",
    "else:\n",
    "    print(\"No overlapping elements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlapping elements found.\n"
     ]
    }
   ],
   "source": [
    "# Extract unique counties\n",
    "unique_counties = trainData_district_filtered['train_10']['District'].unique()\n",
    "\n",
    "# Split the counties into train and test sets\n",
    "train_counties, test_counties = train_test_split(unique_counties, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create train and test sets based on the split counties\n",
    "train_data_district = trainData_district_filtered['train_10'][trainData_district_filtered['train_10']['District'].isin(train_counties)]\n",
    "test_data_district = trainData_district_filtered['train_10'][trainData_district_filtered['train_10']['District'].isin(test_counties)]\n",
    "\n",
    "trainData_district_before = {'train_10': train_data_district[ predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid']]}\n",
    "testData_district_before = {'test_10': test_data_district[ predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid']]}\n",
    "\n",
    "overlap = set(train_counties) & set(test_counties)\n",
    "if overlap:\n",
    "    print(\"Overlapping elements found:\", overlap)\n",
    "else:\n",
    "    print(\"No overlapping elements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlapping elements found.\n"
     ]
    }
   ],
   "source": [
    "# Extract unique counties\n",
    "unique_counties = trainData_subcounty_filtered['train_10']['s1aq5a'].unique()\n",
    "\n",
    "# Split the counties into train and test sets\n",
    "train_counties, test_counties = train_test_split(unique_counties, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create train and test sets based on the split counties\n",
    "train_data_subcounty = trainData_subcounty_filtered['train_10'][trainData_subcounty_filtered['train_10']['s1aq5a'].isin(train_counties)]\n",
    "test_data_subcounty = trainData_subcounty_filtered['train_10'][trainData_subcounty_filtered['train_10']['s1aq5a'].isin(test_counties)]\n",
    "\n",
    "trainData_subcounty_before = {'train_10': train_data_subcounty[ predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid'] ]}\n",
    "testData_subcounty_before = {'test_10': test_data_subcounty[ predictorList + ['FCSStaus', 'covid', 'District_covid', 'county_covid', 'subcounty_covid'] ]}\n",
    "\n",
    "overlap = set(train_counties) & set(test_counties)\n",
    "if overlap:\n",
    "    print(\"Overlapping elements found:\", overlap)\n",
    "else:\n",
    "    print(\"No overlapping elements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time series data for different region levels: before COVID data \n",
    "StdTrain_district_during, StdTest_district_during, SMOTE_Train_district_during, ADASYN_Train_district_during, SMOTETOM_Train_district_during, SMOTEENN_Train_district_during = standardiza_resample_data_reindex(trainData_district_before, testData_district_before,  predictorList, standardizationList, binaryList, freqList, nonstdList)\n",
    "StdTrain_county_during, StdTest_county_during, SMOTE_Train_county_during, ADASYN_Train_county_during, SMOTETOM_Train_county_during, SMOTEENN_Train_county_during = standardiza_resample_data_reindex(trainData_county_before, testData_county_before, predictorList, standardizationList, binaryList, freqList, nonstdList)\n",
    "StdTrain_subcounty_during, StdTest_subcounty_during, SMOTE_Train_subcounty_during, ADASYN_Train_subcounty_during, SMOTETOM_Train_subcounty_during, SMOTEENN_Train_subcounty_during = standardiza_resample_data_reindex(trainData_subcounty_before, testData_subcounty_before, predictorList, standardizationList, binaryList, freqList, nonstdList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_district = {'StdTrain_district_during': StdTrain_district_during, 'StdTest_district_during': StdTest_district_during, 'SMOTE_Train_district_during': SMOTE_Train_district_during, 'ADASYN_Train_district_during': ADASYN_Train_district_during, 'SMOTETOM_Train_district_during': SMOTETOM_Train_district_during, 'SMOTEENN_Train_district_during': SMOTEENN_Train_district_during}\n",
    "data_before_county = {'StdTrain_county_during': StdTrain_county_during, 'StdTest_county_during': StdTest_county_during, 'SMOTE_Train_county_during': SMOTE_Train_county_during, 'ADASYN_Train_county_during': ADASYN_Train_county_during, 'SMOTETOM_Train_county_during': SMOTETOM_Train_county_during, 'SMOTEENN_Train_county_during': SMOTEENN_Train_county_during}\n",
    "data_before_subcounty = {'StdTrain_subcounty_during': StdTrain_subcounty_during, 'StdTest_subcounty_during': StdTest_subcounty_during, 'SMOTE_Train_subcounty_during': SMOTE_Train_subcounty_during, 'ADASYN_Train_subcounty_during': ADASYN_Train_subcounty_during, 'SMOTETOM_Train_subcounty_during': SMOTETOM_Train_subcounty_during, 'SMOTEENN_Train_subcounty_during': SMOTEENN_Train_subcounty_during}\n",
    "# Define the output directory\n",
    "output_dir = 'bld/datasets/generated/'\n",
    "\n",
    "# Store each dictionary as a pickle file\n",
    "with open(f'{output_dir}data_before_district.pkl', 'wb') as f:\n",
    "    pickle.dump(data_before_district, f)\n",
    "with open(f'{output_dir}data_before_county.pkl', 'wb') as f:\n",
    "    pickle.dump(data_before_county, f)\n",
    "with open(f'{output_dir}data_before_subcounty.pkl', 'wb') as f:\n",
    "    pickle.dump(data_before_subcounty, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
